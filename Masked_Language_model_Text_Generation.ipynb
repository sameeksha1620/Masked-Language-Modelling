{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#installing necessary libraries\n",
        "! pip install huggingface_hub\n",
        "! pip install transformers\n",
        "! pip install datasets"
      ],
      "metadata": {
        "id": "_FHWxkmEbmQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgYxawQMahtK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# Log in to the Hugging Face Hub from the notebook environment\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
        "from transformers import TFAutoModelForMaskedLM\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from transformers import create_optimizer, AdamWeightDecay\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import pipeline\n",
        "import math"
      ],
      "metadata": {
        "id": "08GQT290YvzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send example telemetry with specific information\n",
        "from transformers.utils import send_example_telemetry\n",
        "send_example_telemetry(\"language_modeling_notebook\", framework=\"tensorflow\")"
      ],
      "metadata": {
        "id": "1LBGSd0GavdO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the dataset**"
      ],
      "metadata": {
        "id": "6JqQLxAUibyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 'wikitext' dataset from the 'datasets' library\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
      ],
      "metadata": {
        "id": "OqS_UHQtcWt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to display a specified number of random elements from a dataset\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    # Ensure that the number of examples requested is not greater than the dataset size\n",
        "    assert num_examples <= len(\n",
        "        dataset\n",
        "    ), \"Can't pick more elements than there are in the dataset.\"\n",
        "    # Initialize an empty list to store randomly selected indices\n",
        "    picks = []\n",
        "    # Randomly select indices for displaying examples\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset) - 1)\n",
        "        # Ensure that the same example is not picked again\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset) - 1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    # Create a DataFrame containing the randomly selected examples\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    # If a feature is of type ClassLabel, convert the indices to human-readable labels\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    #Display the DataFrame as HTML\n",
        "    display(HTML(df.to_html()))"
      ],
      "metadata": {
        "id": "Dgr_1kWIdKsU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a selection of random examples from the \"train\" subset of the loaded dataset\n",
        "show_random_elements(datasets[\"train\"])"
      ],
      "metadata": {
        "id": "aJJKqQs0evUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128   # Set the block size for tokenization"
      ],
      "metadata": {
        "id": "8arkvlivfV6z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Masked Language Modelling**"
      ],
      "metadata": {
        "id": "LrurdkJxVlGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilroberta-base\"   # Define the model checkpoint to use"
      ],
      "metadata": {
        "id": "h4nkNztNVp5c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tokenization function that takes examples and tokenizes them using the tokenizer\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])"
      ],
      "metadata": {
        "id": "OL8aWgb4WDlF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer instance based on the specified model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# Tokenize the datasets using the provided tokenization function\n",
        "# Map the function across batches, using multiple processes\n",
        "tokenized_datasets = datasets.map(\n",
        "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
        ")"
      ],
      "metadata": {
        "id": "H_KSGf5uVqWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to group tokenized texts into chunks of a specific block size\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts within each example\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "\n",
        "    # Determine the total length of concatenated texts and adjust to multiples of block_size\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # Split concatenated texts into chunks of size block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create 'labels' by copying 'input_ids', since this is used for language modeling\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZjgdunANWls7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the 'group_texts' function to tokenized datasets\n",
        "# Map the function across batches, using multiple processes\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ],
      "metadata": {
        "id": "nWIQpi4pVsgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the masked language model using the specified checkpoint\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "df60HG67WfCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer using AdamWeightDecay with specified learning rate and weight decay rate\n",
        "optimizer = AdamWeightDecay(lr=2e-5, weight_decay_rate=0.01)\n",
        "# Compile the model with the defined optimizer and enable JIT compilation\n",
        "model.compile(optimizer=optimizer, jit_compile=True)"
      ],
      "metadata": {
        "id": "F2PKAwMuXAWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a DataCollatorForLanguageModeling for preparing training data\n",
        "# Set tokenizer and MLM probability, and specify returning tensors in NumPy format\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"np\"\n",
        ")"
      ],
      "metadata": {
        "id": "JgVjYbtxXJPm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training set and validation set using the model's 'prepare_tf_dataset' method\n",
        "train_set = model.prepare_tf_dataset(\n",
        "    lm_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "validation_set = model.prepare_tf_dataset(\n",
        "    lm_datasets[\"validation\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "WDk2NrvXXL2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the model name from the checkpoint path\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "# Train the model on the training set and validate on the validation set\n",
        "model.fit(train_set, validation_data=validation_set, epochs=1, callbacks=[callback])"
      ],
      "metadata": {
        "id": "zZTBit3oXOpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set and retrieve evaluation results\n",
        "eval_results = model.evaluate(validation_set)\n",
        "print(f\"Perplexity: {math.exp(eval_results):.2f}\")"
      ],
      "metadata": {
        "id": "stjBFPj3XSGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**"
      ],
      "metadata": {
        "id": "phvpdL79iqrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for filling masked tokens using the fine-tuned model\n",
        "mask_filler = pipeline(\n",
        "    \"fill-mask\",\n",
        "    \"Rocketknight1/distilroberta-base-finetuned-wikitext2\",\n",
        "    framework=\"tf\",\n",
        ")"
      ],
      "metadata": {
        "id": "JbnmR1S7XW3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Examples"
      ],
      "metadata": {
        "id": "hR0PiVyghT__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler(\"The most common household pets are <mask> and dogs.\", top_k=1)"
      ],
      "metadata": {
        "id": "k2trhxnEXb9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler(\"The Gulf War was a conflict that took place in <mask> in 1990-1991.\", top_k=3)"
      ],
      "metadata": {
        "id": "YKVwxusHXfqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "wNH0G6JohW_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "N0ZaYKWQhZZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define a function that takes an input text and returns the model's predictions\n",
        "def generate_text(input_text):\n",
        "    # Use the 'mask_filler' pipeline to generate masked text predictions\n",
        "    predictions = mask_filler(input_text)\n",
        "    return predictions[0][\"sequence\"]\n",
        "\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=gr.inputs.Textbox(),\n",
        "    outputs=gr.outputs.Textbox(),\n",
        "    title=\"Language Model Text Generation\",\n",
        "    description=\"Enter a sentence with a masked word to see model predictions.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "EIfeqAKZh3uS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}